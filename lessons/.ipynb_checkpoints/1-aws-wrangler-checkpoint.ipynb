{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25660cad-3121-4f24-a789-898745b588e7",
   "metadata": {},
   "source": [
    "# AWS Wrangler Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9ebd2-3de6-4371-8727-80f1869e79ba",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38074c08-be85-43bb-82e8-69786c0b8916",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice working with the AWS wrangler library to both read and write data to our s3 resource. \n",
    "\n",
    "You can view the documentation for this library [here](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7384bbd-73f2-49a2-93ab-0aa103e0d6d7",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6883407-bf46-4ebf-ace4-dddb9acf85ae",
   "metadata": {},
   "source": [
    "In this lesson, we'll be working with the ecommerce data located in the `data` folder.\n",
    "\n",
    "You can see that we provided you with a function to read the data, `get_data`, which you can see in the `index.py` file.  You can call that function from the `console.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4979a-3074-457e-8819-9cc67e18bbbf",
   "metadata": {},
   "source": [
    "### Writing to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4aca3-b8bd-4ae3-9fba-112f5930f483",
   "metadata": {},
   "source": [
    "Now create an s3 bucket from the aws console.  \n",
    "\n",
    "* `write_to_s3(df, path)`\n",
    "\n",
    "Next write a function called `write_to_s3`, that takes in arguments of a dataframe and the s3 path to the bucket.  Given those to arguments, the function should write to the bucket, storing in parquet format, and writing as a dataset.\n",
    "\n",
    "Notice at the top of the function, we placed a line of `boto3.setup_default_session(region_name=\"us-east-1\")`.  Make sure that the region name matches the location of the bucket.  You can find this, by viewing the s3 buckets in the console.\n",
    "\n",
    "<img src=\"./aws-region.png\" width=\"80%\">\n",
    "\n",
    "> **Remember** that writing as a **dataset** means that the files will be written to the folder generally, and then be read from that folder without specifying the particular file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bf95c-003e-4617-b301-2f047158f6a1",
   "metadata": {},
   "source": [
    "* `read_from_s3`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc251982-1248-4d9c-955c-433122a12337",
   "metadata": {},
   "source": [
    "Write a function called `read_from_s3` that provided a path will to the bucket, will return a dataframe of our data.  \n",
    "\n",
    "> This function should also start with that line of: \n",
    "\n",
    "`boto3.setup_default_session(region_name=\"us-east-1\")`.  Make sure that the correct region is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983987fa-37de-4d73-9ec9-adeabed0d1c1",
   "metadata": {},
   "source": [
    "4. Partition\n",
    "\n",
    "Now let's partition the dataset.  Update the `write_to_s3` method so that `partition_cols` argument is added.  And partition by the date.\n",
    "\n",
    "Then try calling the `write_to_s3` data again.  This time if you visit the bucket, you should see that the data is partitioned by date.\n",
    "\n",
    "<img src=\"./partitioned-data.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801b2fa-52f8-4610-8dac-9f4a954de313",
   "metadata": {},
   "source": [
    "### Creating a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf67c7c-dbd1-4f4d-959a-069d8e7104f3",
   "metadata": {},
   "source": [
    "Now, let's use AWS glue to bring some order to our datalake.  Begin by using wrangler to list the available databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a69643f-6417-4e60-97e9-d510ed7a12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = wr.catalog.databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adc6ff-eafd-4ae6-91ac-3e73706f30ba",
   "metadata": {},
   "source": [
    "Then create a new database called `ecommerce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87683653-80a2-4792-8eab-2bc3d0a8dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.catalog.create_database('ecommerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3945e-3dba-4c1a-90bf-250f54ab37e5",
   "metadata": {},
   "source": [
    "Finally, use the `store_parquet_metadata` function to scan the proper bucket and allow us to read the data as a table from the ecommerce database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cac52-b3e4-46cd-a687-83f052a630c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 's3://jigsaw-wrangler'\n",
    "\n",
    "res = wr.s3.store_parquet_metadata(\n",
    "    path=path,\n",
    "    database=\"ecommerce\",\n",
    "    table=\"sales\",\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e4cd3-cf4f-4d44-98c4-ac1d1f96bf9c",
   "metadata": {},
   "source": [
    "From there, write a function called `read_from_db` that takes in an argument of query, and will then return the results of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6faf52-814f-4a1f-9ad1-9a9c751b5974",
   "metadata": {},
   "source": [
    "> You may have to have the `boto3.setup_default_session(region_name=\"us-east-1\")` at the top of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c63173-5f6a-473e-8024-c4122f9293cd",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Documentation](https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.s3.read_parquet.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
